[ { "title": "Winning a Children&#39;s Game with Combinatorial Graph Theory", "url": "/posts/SET/", "categories": "Blogging, Explorations", "tags": "computer science, story, math, graph theory, combinatorics, programming", "date": "2022-04-17 19:23:00 -0600", "snippet": "Playing a Children’s GameRecently a LinkedIn connection indirectly introduced me to this game called SET.The rules are simple enough. 12 cards are dealt, all of which have images with four properties: color, number, shape, and texture. The challenge is to find sets of three cards such that, for each property, either all cards are the same or all cards are different.For example, in the above round, one such set is: Three hollow green diamonds. Three filled red diamonds. Three shaded purple diamonds.In this case, the numbers and shapes match, but none of the textures nor colors match. It’s worth noting here that it’s possible with 12 cards to have no valid sets.I spent about two minutes trying to find a few sets in my first game before declaring, “this is silly, I’m going to write a program to solve it for me.” So that’s exactly what I did.Winning a Children’s Game with PythonThe first order of business is figuring out how to represent a dealing of cards in code. My first instinct was to convert all the cards to dictionaries storing the values:{ &quot;number&quot;: 3, &quot;texture&quot;: &quot;hollow&quot;, &quot;color&quot;: &quot;green&quot;, &quot;shape&quot;: &quot;diamond&quot;}Unfortunately, this gets unwieldy quite fast, since typing out twelve cards like this takes quite a bit of work. Soon enough, I realized that we didn’t need to store the full names of the card’s attributes, but only a letter. And we didn’t need to specifically declare what type of attribute it was, we could use an order to distinguish that. So we could encode 3 hollow green diamonds as 3hgd. Now we’re in business, our earlier example game can be encoded as:game = [ &#39;3hgd&#39;, &#39;3hgo&#39;, &#39;1fro&#39;, &#39;3spd&#39;, &#39;2frd&#39;, &#39;1sps&#39;, &#39;2spo&#39;, &#39;3frd&#39;, &#39;1hgo&#39;, &#39;1frs&#39;, &#39;2fpo&#39;, &#39;3fgd&#39;]Our cards now solidly written in code, the easiest way to continue is to try every possible triplet (using itertools) and see which ones work.def compare(card1, card2): # For each attribute, we only care if they&#39;re equal. # Simply check for equality pairwise. return [a == b for a, b in zip(card1, card2)]def is_set(card1, card2, card3): # The attributes must all be equal or all be unequal, # test by checking every pair. return compare(card1, card2) == compare(card2, card3) == compare(card1, card3)def all_sets(game): # Check every (unordered) combination of 3 cards. for triplet in itertools.combinations(game, r=3): if is_set(*triplet): print(*triplet)Running this on our above game, we get:3hgd 3spd 3frd3hgd 2spo 1frs3hgo 1fro 2spo3hgo 2frd 1sps3spd 1sps 2spo1frs 2fpo 3fgdTelling us that our game has six valid sets. A quick sanity check shows that our earlier example set for this game is in there, 3hgd 3spd 3frd. It turns out there’s only $\\binom{12}{3}=220$ triplets per game to check, so our code runs quite quickly: on my machine it can find a valid set in about 8000 games per second!…But what if we could go faster?Winning a Children’s Game with CombinatoricsThere’s a peculiar property about the game that lets us shave off quite a bit of checking: given any two cards, the third card is fixed.To see this, consider the cards 1fgd and 2hgd. For there to be a third card that forms a set with these, it must not share attributes with 1f and 2h since they’re different, and it must share attributes with gd because they’re the same. So the third card must be 3sgd.Since the third card is fixed, that means that we can avoid checking every triplet. Instead, for every pair of cards, we only need to check if the third card has also been dealt. To start, we need to program a method to calculate the third card.It turns out this is actually quite tricky with our current notation: we need to separately treat that the other values of 1 are 2 and 3 and the other values of h are f and s. To remedy this, we can simplify even more: it doesn’t strictly matter what the exact values of the attributes are, so we can use the same convention for all of them. Let’s use the digits 1, 2, 3 for all of them, and then every card is a 4-digit number. The set from right above can then be encoed as, for example, 1123 2223 3323.With this, the third matching card can be calculated directly, which is quite easy using Python’s generator syntax and built-in set difference:def match(card1, card2): return &#39;&#39;.join( ({&#39;1&#39;, &#39;2&#39;, &#39;3&#39;} - {a, b}).pop() if a != b else a for a, b in zip(card1, card2) )def all_sets(game): for pair in itertools.combinations(game, r=2): if match(*pair) in game: print(*pair, match(*pair))As it stands, this will print out each triplet three times, one for each missing card. This can be fixed by checking if a found set has already been included, but it’s not too important. It’s actually a little more interesting to treat the game as “find at least one set” than “find all sets,” so let’s just do a light change:def has_set(game): for pair in itertools.combinations(game, r=2): if match(*pair) in game: return True return FalseCompared to the decision version of the above brute-force algorithm, this solves 35000 puzzles per second, which is a solid performance improvement of about 440%.But maybe we could go faster?Winning a Children’s Game with InterpolationOkay, this one isn’t that bad, but the name does sound really cool. In the above code, pay attention to this part: ({&#39;1&#39;, &#39;2&#39;, &#39;3&#39;} - {a, b}).pop(). The goal is to find which of the three attributes we’re missing in our set of two, so we create a set of all three attributes, remove the two we have, and take (pop) what remains. This works fine, but for what it’s doing, it’s quite expensive.We can directly look at the ASCII values of our characters here: 1 has an ASCII value of 49, 2 of 50, and 3 of 51. To calculate the third character given two characters, we can express this as a linear system of equations to see what function we need:\\[\\begin{aligned}49a+50b+c&amp;amp;=51\\\\49a+51b+c&amp;amp;=50\\\\50a+51b+c&amp;amp;=49\\end{aligned}\\]Plug it into your favorite symbolic calculator and we get $a=b=-1$ and $c=150$. This means we can calculate the third value by:def match(card1, card2): return &#39;&#39;.join( chr(150 - ord(a) - ord(b)) if a != b else a for a, b in zip(card1, card2) )Comparing this to the previous one, now we solve 58000 puzzles per second, a nice 165% improvement.Now I know what you’re thinking: “Surely there must be a way to remove that if statement and compute the appropriate value in one go.” There is! But it’s both uglier and less efficient, so let’s move on.Winning a Children’s Game with Combinatorial Graph TheoryWe finally get to the original title of this post. I’ll be assuming from here that you have an acute familiarity with Graph Theory.We can model SET as a complete graph with 12 vertices, one for each card. We can model the relations between cards using edge colorings. We’ll assign one of 15 “colors” to each edge in the graph, and the color is defined by how similar the cards are. Using an operation resembling a bitwise XOR, the color can have a 1 in a position if the attributes have a different value and 0 if they’re the same. So the color of the edge between 1123 and 1131 would be 0011. Given all relationships and the fact that no two cards are the same, we have each edge receiving one of 15 colors, which are really just bitstrings here.Now our problem is just a matter of determining whether our complete graph contains a monochromatic clique of size 3. A clique is a subgraph where all vertices are directly connected (guaranteed with a complete graph), and a monochromatic clique is one where all of the edges are the same color.Now, unfortunately I only knew enough graph theory to know that [Ramsey Theory] was involved somehow. After some time trying to see if I could do any better than the earlier $O(n^2)$ to find a set, this is where I wound up turning to the internet for help. Sure enough, after some searching, I came across this paper which identifies the related $\\text{AE-Mono}\\Delta$ problem, which is a super cool name.After following some citations for a while I land at this other paper which actually looks at this problem! It concludes that the algorithm is solvable in… drumroll… $O(n^{\\frac{3+\\omega}{2}})$ time. Now, I was quite excited when I first saw this, as it means that we can get below $n^2$ if $\\omega$ is sufficiently small. Unfortunately, $\\omega$ turns out to be the exponent for fast matrix multiplication, a certain galactic algorithm. $\\omega$ is strictly lower-bounded to be at least $2$, so that’s no dice, we need to abuse further properties of the structure of the problem to hope to get any faster than $n^{5/2}$, which we already did above!So, that didn’t really work. But oh well, part of the fun of math is that simple questions about children’s games can lead to unsolved problems in graph theory.But, one of the authors of that paper with the $O(n^{\\frac{3+\\omega}{2}})$ result, Raphael Yuster, has an Erdős number of 2. Since I’m the first person that I can find who connected their work to algorithms for solving SET as a decision problem, I’m going to proudly claim my updated Erdős number of 3 (down from 5)." }, { "title": "Data Mining my Favorite Game", "url": "/posts/Joseki-Scraping/", "categories": "Blogging, Explorations", "tags": "computer science, data mining, go, story", "date": "2022-02-17 20:31:00 -0700", "snippet": "At this point it’s probably no secret that I love Python. Slightly better-kept is the fact that I love this game called Go. It saw a bit of a surge in popularity in the west after DeepMind’s AlphaGo beat one of the top players of the time, myself being one of many to hop on the bandwagon of new players.It’s somewhat rare for the two hobbies of programming and Go to intertwine; however, you’ve seen the title, so you know that I’m going to be applying some data mining to this game. Yes, you’re right. But before then, I need to set the stage for what data I intend to mine, and why it matters.Introducing GoAt its core, Go is a game about efficiency. I’m also a programmer and a math hobbyist, so it’s not too hard to see why I might be pulled into it. The goal is to surround territory: you win if you surround more points than your opponent. Consider the start of this game:Both players have placed four stones on the board, and with those stones, they’ve started to claim some space. Black seems to largely be asking for the top of the board, while white is largely asking for the bottom. There’s a lot of uncertainty in the position, but this is what we would expect. If we follow the simplest sequence of events, where both players get exactly what they ask for, we get something like this at the end:In this game, White has surrounded 23 points of territory ($9\\times2+5$) while Black has surrounded 28 points of territory ($9+7+6\\times2$). All else being equal, Black won the game by five points. However, if you look closely, you’ll see both sides have the same number of stones on the board. This means that, in some sense, Black played more efficiently. A lot of the game boils down to getting points with as few stones as possible.On a full-sized 19x19 board (as opposed to the 9x9 above), there’s a lot of known sequences that are considered roughly equal for both sides. That is, when both players play in a certain way, there’s an understanding that both are getting somewhat equal compensation for what they put in. For example, consider this board, particularly in the top left:In this position, both players asked for two corners, and then Black took a slice out of White’s top-left corner. They also both do it with only three stones, which is fairly efficient. The top-left is an example of one of these sequences: the corner is split roughly in half, and both players are satisfied with the result. These sequences are known as joseki, and while they’re far from the most important aspect of the game, they demystify a lot of the early game.These joseki are the primary focus of our project today. It’s important to know at least a few of them when playing, and naturally, the first question a beginner might ask is, “which ones do I learn?” Now, there are a lot of resources on what joseki to learn and how to learn them. But a natural next question is, “which joseki do others play?” This is the main question I’m going to strive to answer, because knowing which joseki are most common can be a valuable teaching tool.To Data-mine a GameIn order to get data on joseki played in games, we need games. There are many databases online for professional Go games, played by some of the strongest players the game has to offer. These are really valuable resources, but they’re not of interest to me. First, because many people have done a version of this on pro games, and second, because I am not a pro. I want to know what is most often played by amateurs like myself, and that requires studying amateur games.The database I’m using is from the Online Go Server, which has over 25 million games stored in about 80 gigabytes from 2013 to now. Something to note is that joseki will often go in and out of fashion, so I’m mostly interested in somewhat recent games, say from 2021 onwards. So the question has transformed from an abstract one of “which joseki do people play most often,” to “how do I extract sequences from 80 gigabytes of games?”This question has two main parts: extracting sequences, and 80 gigabytes of games. The former is a challenge on its own, because it’s nontrivial to find sequences in games. Consider the previous image and this one:If you direct your attention to the top left again, you’ll see the same shape as before. However, not only is there more on the board, making recognition of the sequence more challenging. The sequence is also at a different orientation. This means that we need to not only figure out how to work out symmetry and timing, but also filtering the global position to only consider regions of interest.The second part of the challenge, the fact that the database is 80 gigabytes, is also difficult. It’s large enough that we need to batch our processing, and ensure our analysis is somewhat efficient. So let’s tackle the problems one at a time.Filtering Patterns from NoiseWhen presented with the challenge and the two examples above, one’s first instinct may be to set a distance threshold. Record moves that are within certain distances of each other, and analyze the resulting shapes. This is insufficient: moves can be somewhat far away and still part of joseki, and interference between patterns can be somewhat common. Attempting to work out this inconsistency threw me off for a few days, until eventually I found that someone else put more effort into the same problem.The heroes of this story are Carson Leung et al. from the University of Manitoba, and their paper Data analytics on the board game Go for the discovery of interesting sequences of moves in joseki. He prescribes an algorithm (without providing any code) that goes like this: Maintain a list of four joseki sequences, one per corner. For each move, add it to one of the sequences if conditions are met. The distance is sufficiently small The sequence doesn’t already have more than 20 moves (who wants to memorize that much?) The move isn’t the same distance from multiple joseki Otherwise, add it to a fifth “Not Joseki” list, which is used in other computations.The algorithm is a bit complex, but translating it to code turns out to be somewhat straightforward, and performant too. It processes about 400 games per second on my machine. Once the list of joseki is extracted, the next phase is merging them together, removing symmetry (de-reflecting) and tracking frequencies, into one large game tree. This relies on one of my favorite data structures, the Trie. Although in practice I just used a nested json object for easy serialization.Once it’s merged, we can just take the resulting Trie, prune it by some threshold (e.g. we might be only interested in joseki that appear in 1% of games or more), and save it to SGF (the serialization format for Go).A note on the dataNow, unfortunately we can’t just run this directly on the multi-gigabyte games file: we need some form of batching things. To do this, I decided to process the file one game at a time, only tracking those games which met our criteria. The filtering process when completed could handle about 20000 games per second, which is still a sizeable amount of time with over 25 million games, but it worked. Fortunately the data is still small enough that after preprocessing, I could put it into one batch.There were a few filtering criteria I used. First, I restricted the set to only games with players of a certain strength: too weak and the moves quickly start to depart from balanced sequences. Additionally, I only considered non-handicap 19x19 games, as they’re the games where standard joseki are applied the most. Finally, as mentioned, I only considered games during 2021.ResultsSo, what is the most common joseki? Unfortunately, I already revealed it earlier, twice. The six-move sequence I showed is on the top of the list. I suppose it makes sense that I happened to choose that one, then. But you can see the rest of it in my upload to OGS here, sorted by frequency. Additionally, the full code is available on my Github.As an aside, you should check out Go. It’s a fun game." }, { "title": "Breaking Hashes, Part I. What is a broken hash?", "url": "/posts/Breaking-Hashes-I/", "categories": "Blogging, Explorations", "tags": "cryptography, math, arrays, number theory, computer science, hash functions", "date": "2021-11-29 15:20:00 -0700", "snippet": "I’ve long been interested in hash functions. It’s hard to point to a specific reason why, but there’s something interesting about the fact that you can compute a deterministic value for a given input, and that you can’t easily find what the original input was. It’s not hard to imagine how something like this could exist, but I’ve long been interested in the specifics. What specifically makes a given hash function cryptographic or non-cryptographic? What tools exist for breaking them? What do existing cryptographic hash functions do well?In particular, one family of functions that grabs my attention are the Secure Hashing Algorithms (SHA), and specifically the SHA-2 family of hash functions. This is the family behind the often-used algorithms SHA256 and SHA512. If you google “Why is SHA-2 secure,” the answer is unfortunately quite bare: it’s designed to be so. Nobody seems to answer in digestible terms the mechanisms that make these particular functions secure. Why can’t we break it? What types of attacks are known, and why do they fail? For the past while I’ve sought to answer these, looking around but failing to find anything that satisfied my curiosity.In a multi-part series of posts, I plan to explore hash functions and methods for breaking them, ultimately getting to why currently used cryptographic hash functions haven’t been broken. In this post I’m going to start us on that somewhat lengthy path by going into what hash functions are and what types of attacks cryptographers care about.An Introduction to Hash FunctionsLet’s begin with a story that closely parallels the origins of hashing. It’s the early 1950s, and numbers are starting to become a lot more important in daily life. Social security numbers, credit cards, barcodes, there are a lot of cases which rely on having a number (which is unwieldy to memorize). One issue that you consistently face is fakes: people like to tamper with credit cards, social security, and barcodes, for an assortment of nefarious purposes. You want to be able to check if something is valid. This is the point where you might want pause and ponder if you’ve not been exposed to the idea before.Some thought might lead you to the following idea. If you ultimately want to know if a number is valid or not, you want to assign some semantic information to a number. That information might be added to the end of the number, and look like 123-45-6782 VALID or 123-45-6783 INVALID. Since validity has two states, it can be a little more wieldy to just write 0 for valid, and anything else (like 1) for invalid (this might seem weird, but having one value for valid and many for invalid turns out to make things easier). However, if we try to implement this directly, maybe by having a big list of numbers and if they’re 0 or 1, we’ll quickly run into storage issues. Instead, we want to be able to look at a number and figure out if it’s valid or invalid entirely on its own merit.When Hans Peter Luhn approached this problem in the early and mid 1950s, he invented the following method: Take your number 123-45-6782 Double every other digit in-place, meaning we don’t care about overflows: 143-85-127162 Add the digits, then multiply by 9: 40, 360 Check if the last digit is valid, that is, 0. We see that our number was valid.When we run the same process on the earlier 123-45-6783, we get a 9, indicating that the number is invalid. One further leap in logic, and we might try using the last digit of the number as our validity check: instead of our calculation on all values, we can find what happens if we just do the function on 123-45-678. As it turns out, we get 2, indicating that the last digit of the number should be 2 for a valid result. Now we can check validity by seeing if the last digit is 2.This, as it turns out, is a simple version of what is called a hash function. We use the data we have to compute a new result (called a hash or checksum), and we can use that result to authenticate our data. Now trivially tweaking the data won’t work: if we hastily try to spoof a new social security number, for example with 123-45-6582, we’ll find that our hash doesn’t check out.In many applications, hash functions are used for some type of authentication. We want to know if the data we received is valid, without having to store the data somewhere to look it up later. This is what happens to your password when you log in to any service worth its salt, or use an authenticated encryption protocol. Only, instead of a digit from 0 to 9, we’re looking at a number from 0 to 115792089237316195423570985008687907853269984665640564039457584007913129639935 ($2^{256}-1$), or some other absurdly large power-of-2-based number.For many applications, we want a cryptographic hash function. For a hash function to be cryptographic, it should be resistant to both pre-image attacks and collision attacks. I’ll cover both of these in the next section, and show that Luhn’s method isn’t cryptographically secure because it’s susceptible to both of these.When is a Hash Function Broken?One issue that you may have noticed with the earlier scheme is that we’ve barely stopped people tampering with our numbers. Someone could easily just guess random numbers until it works. However, this relies on the small outspace (from 0 to 9 here). However, for practical purposes the output will be way larger, and we’ll need to be more clever. Unfortunately, that doesn’t save our friend Luhn. To demonstrate this, let’s start by writing some python code to compute hashes per his method:def luhn_hash(number): # Retrieve all digits from our string digits = [int(i) for i in number if i.isdigit()] # Double every other digit doubled = [d*2 if i%2&amp;gt;0 else d for i, d in enumerate(digits)] # Add up all the total digits digit_sum = sum(map( lambda x: sum(map(int, str(x))), # Nested sum for 2-digit results doubled )) # Multiply by 9 and extract the last digit return (digit_sum * 9) % 10And now let’s compute some hashes:&amp;gt;&amp;gt;&amp;gt; luhn_hash(&#39;123-45-678&#39;)2&amp;gt;&amp;gt;&amp;gt; luhn_hash(&#39;223-45-678&#39;)1&amp;gt;&amp;gt;&amp;gt; luhn_hash(&#39;223-46-678&#39;)0&amp;gt;&amp;gt;&amp;gt; luhn_hash(&#39;224-45-678&#39;)9&amp;gt;&amp;gt;&amp;gt; luhn_hash(&#39;225-45-678&#39;)8You might notice a pattern: if we increase an even-indexed digit in the input by 1, the value decreases by 1 (wrapping around at 0). Using this rule, it turns out we can easily generate an input that corresponds to any output we want: take any even-indexed digit, and increment it until we get out desired output. A simple algorithm might look something like: Compute the hash. Find the offset of our current hash from our target hash. Increment the first digit by that much (wrapping if necessary).After implementing it and verifying it works, we’ve accomplished the goal: for a given hash, we can quickly generate an input that has that hash. This is called a pre-image attack. This also means that if the receiver is expecting a specific hash for the data, we can tweak the data arbitrarily to ensure that it looks valid.Another way we might want to break a hash is to find two values that look the same, without necessarily targeting any specific hash. In essence, find two values whose hashes collide, in what’s called a collision attack. This is a bit less versatile than a pre-image attack, but it still has its uses (such as in digital signatures). It turns out a collision attack is often easier than a pre-image attack: for example, md5, a widely broken algorithm, has widespread collision attacks, but is still considered pre-image resistant.In this case, a collision attack does turn out to be easier than a pre-image attack: by using our earlier rule of an increment in an even index decrementing the hash, we can quickly generate collisions by taking any fixed input, 000-00-000, and incrementing two different indices, yielding 100-00-000 and 001-00-000.So now we have two types of attacks that we can use to “break” a hash function. When we get to dissecting SHA-2, we’ll be looking at both of these, and seeing why exactly the function is so difficult to crack. Before that, however, in the next part I plan to look at breaking more non-cryptographic hash functions, to survey the scene.Unimportant Digressions The term pre-image attack was confusing to me for a while, until one day it suddenly clicked. Mathematically, an image of a function on a set is the outputs you get when applying that function on every element of the set. For example, the image of $f(x)=x^2$ on $\\{2, 3, 4\\}$ is $\\{4, 9, 16\\}$. The pre-image of a function is then the inputs that produce those outputs, so the pre-image of $f$ on $\\{4, 9, 16\\}$ is $\\{2, -2, 3, -3, 4, -4\\}$. The term pre-image attack then makes sense: given a cryptographic hash function, we want the pre-image of the function on a given output hash. An often-stated requirement for a cryptographic hash function is the avalanche effect: for a small change in input, the output should change significantly and sufficiently randomly. This is a big topic, and proving the avalanche-ness of a function turns out to be quite difficult (see: strict avalanche criterion, bit independence criterion). I’ve brushed over it since it is closely related to pre-image resistance, and isn’t too important for where we’re going." }, { "title": "Descending Through Arrays and Problem Solving", "url": "/posts/Descending-Arrays/", "categories": "Blogging, Problems", "tags": "problem solving, math, arrays, graph theory, number theory, computer science, story", "date": "2021-11-15 12:00:00 -0700", "snippet": "A Mystery is AfootLike many of the best stories in mathematics, our story today begins with playful pondering. We could be thinking of any number of things: a video game, a chore, opening presents for the upcoming Christmas, opening presents in a certain order, visiting indices of an array…Well, there’s an interesting thought. What if our Christmas presents contained a pointer to the next present in a sequence? Thinking more in the language of computer science, we might have an array containing $n$ integers, where each integer represents another index to visit next. Maybe there’s a target we want to visit? Our Christmas box with our favorite wrapping paper, that may have our favorite gift? Let’s try organizing our array so that the target is to the far left, and doesn’t point anywhere. Let’s also make it so that we start at the right. Finally, we can add a condition that every box randomly points to some box to its left. So our setup looks like this:Now we’re getting somewhere. If we start at the end of the chain and continually follow our references, we’ll eventually wind up at our target.But just one chain is boring: I want to know about multiple chains. What if I really want to get to the left quickly, but I end up having to go through every box? I wouldn’t like that very much. It seems natural to ask, then, what the average behavior is. Specifically, I’m interested in the number of hops: if I repeat this process indefinitely, what is the average number of hops I’ll go through?More formally, we can say: We’re given an array of $n$ values, each one (except the first one) randomly containing an index less than its own index. Starting at the last item and traversing the chain back, what’s the expected amount of hops, $h(n)$, to get to index 0?Well, this seems like an interesting math problem. It at least captured my attention when I first thought it up. Surely it’s not novel, it seems like the type of thing that would be studied to death. But just seeing the answer is boring, I want to discover it for myself, and become a better problem-solver in the process.Of course, I also encourage you to try the problem for yourself, before seeing my approach. You might also try pausing during the investigation if you spot where I’m headed.The Crime SceneI’m a programmer: when I see a problem where I can simulate the situation, my first instinct is to give it a shot. I began my investigation by writing some code to do exactly that in my language of choice, Python.import random# Let&#39;s try a few counts of items in our listfor n in range(3, 10): # Populating our list with left-facing indices, except the leftmost. ls = [0] + [random.randint(0, k-1) for k in range(1, n)] hop_count, index = 0, len(ls) - 1 # Traversing down the chain while ls[index] != index: hop_count += 1 index = ls[index] # Output our list and the hop count print(ls, hop_count)Running this gives us some output that we can use to check for correctness:[0, 0, 0] 1[0, 0, 0, 1] 2[0, 0, 1, 0, 0] 1[0, 0, 0, 2, 0, 3] 3[0, 0, 0, 1, 2, 3, 0] 1[0, 0, 0, 2, 2, 0, 2, 0] 1[0, 0, 0, 1, 2, 3, 1, 2, 1] 2Confident that our simulation works, we might then try answering our original question empirically, using the highly rigorous method of “Run it thousands of times.”import randomimport statisticsdef trial(n): # The above code, modified to only return the hop count for a single runfor i in range(3, 10): # Get the mean of 100,000 trials for the current i average = statistics.mean(trial(i) for j in range(100000)) # Apply reasonable rounding and output print(i, round(average, 3), sep=&#39;\\t&#39;)Running this gives us some numbers:3 1.4994 1.8355 2.0866 2.2837 2.4498 2.5899 2.716Now, that’s a start, and is in-line with our trial run which mostly yielded low numbers. But, I don’t see any clear pattern here. Some more careful pondering seems necessary. Our initial survey of the crime scene complete, we move on to more intensive forensic analysis.A Familiar FingerprintAs I pondered this, a familiar idea crept into my mind. I’d solved a few similar problems in the past, and that experience gave me an intuition. When looking at successive choices of random values, it can be helpful to choose the “middle” of the distribution. If we consider just one case where we’re moving to a randomly selected spot to the left, on average, we’ll land in the middle of that selection.It seems reasonable that over one hop, we would cut the list in half, on average. Then after another hop, on average we would cut our initial list into a quarter. This type of halving comes up a lot in computer science: for example, in binary search or merge sort. When we see this type of halving, it’s often indicative of logarithms being involved somehow. With binary search we have that our complexity from this halving is logarithmic, $O(\\log_2(n))$, and with merge sort we also have a logarithm in the complexity, $O(n\\log_2(n))$. The additional $n$ term comes from the fact that we’re working on every partition instead of just one.From this, it seems reasonable to guess that our mystery function is $h(n)=\\log_2(n)$. Let’s give it a shot! Modifying our code yet again…import math# ...for i in range(3, 10): mean = statistics.mean(trial(i) for j in range(100000)) # A guess of log base 2 guess = math.log(i, 2) print(i, round(mean, 3), round(guess, 3), round(mean - guess, 3), sep=&#39;\\t&#39;)This gives us an output of:3 1.498 1.585 -0.0874 1.832 2.0 -0.1685 2.086 2.322 -0.2366 2.279 2.585 -0.3067 2.453 2.807 -0.3548 2.591 3.0 -0.4099 2.721 3.17 -0.449Well, that’s unfortunate. The results tell us that our guess consistently overestimates the average, and also that it gets worse as $n$ increases. Well, being wrong sucks, but that also means there’s a hole in our knowledge that we can patch up!Since guessing the solution didn’t work, we need to perform a more careful analysis.A New SuspectSince simply taking the midpoint of what remains in the list didn’t work, we need to concretely look at what we mean by “average.” Well, we know the usual formula for an average in school: if we have a list of items $L=\\{1,1,2,3,5,5,7\\}$ and we want its average, we can take the sum of the list and divide it by the number of items in the list:\\[\\overline{L}=\\frac{\\sum L}{|L|}\\]But how do we apply this calculation to more wild situations, like our array chains? Well, we’re uniformly selecting one of the next indices, and each of those indices surely has its own average. Couldn’t we just take the average of averages?By doing this, we can set up a recurrence relation. We have a base case that we know with a list of 1 index: since we’re already at the start in that case, we have 0 hops. Then every index after that should just be one hop, plus the average of everything that comes after it:\\[h(n)=\\begin{cases}0&amp;amp;n=1\\\\1+\\frac{1}{n-1}\\sum_{i=1}^{n-1}h(i)&amp;amp;n&amp;gt;1\\end{cases}\\]This type of mathematical notation can be pretty confusing without some experience, so let’s also get this translated into code:# ...# Our new guess for the count, based on averagingdef h(n): if n == 1: return 0 else: # Computing the sum of every h(i) below the current one sum_of_left = sum(h(i) for i in range(1, n)) # Computing our average, and adding one hop return 1 + sum_of_left / (n - 1)for i in range(3, 10): mean = statistics.mean(trial(i) for j in range(100000)) # Our new guess guess = h(i) print(i, round(mean, 3), round(guess, 3), round(mean - guess, 3), sep=&#39;\\t&#39;)After mulling over this result and running it, we see:3 1.501 1.5 0.0014 1.832 1.833 -0.0015 2.081 2.083 -0.0026 2.288 2.283 0.0047 2.447 2.45 -0.0038 2.594 2.593 0.0019 2.719 2.718 0.002Progress! Our function seems to closely match the prediction, much better than our initial guess. We might be tempted to call it quits here, but there are still questions to ask. For instance, does this function have a more elegant representation? Let’s continue to investigate.It took me quite a while to spot, but after turning the problem in our heads a bunch, we might happen to notice that the result of our function will always be a fraction. We compute it using only sums and divisions. It seems like it could be productive to see what those exact fractions are, so let’s give it a shot. Modifying our code a little lets us do this:from fractions import Fractiondef h(n): if n == 1: # Fractions will propagate up the call stack with arithmetic operations return Fraction(0) else: sum_of_left = sum(h(i) for i in range(1, n)) return 1 + sum_of_left / (n - 1)for i in range(3, 10): guess = h(i) print(guess)Which yields:3/211/625/12137/6049/20363/140761/280Now, the mathematically inclined might recognize this sequence on the spot, but I sure didn’t. So I turned to the resource I always turn to when I see a curious sequence, the Online Encyclopedia of Integer Sequences, copied the numerators, and…Eureka!With just a few terms, the OEIS guessed the rest of the sequence. It looks like our values are the harmonic numbers $H_n$, shifted down by one. This gives us a new guess:\\[h(n)=H_{n-1}=\\sum_{i=1}^{n-1}\\frac{1}{i}=\\frac{1}{n-1}+\\frac{1}{n-2}+\\cdots+\\frac13+\\frac12+1\\]We might double-check our correctness by implementing it in code:def harmonic(n): return sum(1/i for i in range(1, n+1))for i in range(3, 10): print(h(i) - harmonic(i - 1))And running this verifies that they do indeed return the same output. This gives us a prime suspect, but how can we be sure?Making our CaseThere are a few ways that we might go about proving that our recurrence relation and the harmonic numbers are equivalent. When I first approached the task, I went on an algebraic crusade that involved a lot of difficult leaps in logic, eventually winding up at the conclusion that they were equivalent.But math at its best, much like software engineering, is collaborative. So I turned to some friends to ask if they could find a better approach.Sure enough, one of them did. Compared to my hard algebra, they found a very pretty way to reorder and substitute the two functions, which lets us reach a known property of the harmonic numbers, that $H_n=H_{n-1}+\\frac1n$. It goes like this:\\[\\begin{aligned}\\sum_{i=1}^{n-1}H_i&amp;amp;=\\left(1+\\frac12+\\frac13+\\cdots+\\frac1{n-1}\\right)+\\left(1+\\frac12+\\frac13+\\cdots+\\frac1{n-2}\\right)+\\cdots+1\\\\&amp;amp;=(n-1)+\\frac{n-2}{2}+\\frac{n-3}{3}+\\cdots+\\frac{1}{n-1}\\\\&amp;amp;=\\left(n+\\frac{n}{2}+\\frac{n}{3}+\\cdots+\\frac{n}{n-1}\\right)-\\left(1+\\frac{2}{2}+\\frac{3}{3}+\\cdots+\\frac{n-1}{n-1}\\right)\\\\&amp;amp;=nH_{n-1}-n+1\\end{aligned}\\]Now we consider the equation for $h(n)$ from our earlier equivalence, and increment $n$:\\[\\begin{aligned}h(n)&amp;amp;=1+\\frac{1}{n-1}\\sum_{i=1}^{n-1}h(i)\\\\h(n+1)&amp;amp;=1+\\frac{1}{n}\\sum_{i=1}^{n}h(i)\\\\\\end{aligned}\\]Since we suppose that $H_{n-1}=h(n)$, then we also have $H_n=h(n+1)$. Substituting:\\[H_n=1+\\frac{1}{n}\\sum_{i=1}^{n}H_{i-1}\\]And since $H_0=0$,\\[\\begin{aligned}H_n&amp;amp;=1+\\frac{1}{n}\\sum_{i=1}^{n-1}H_{i}\\\\H_n-1&amp;amp;=\\frac{1}{n}\\sum_{i=1}^{n-1}H_i\\\\nH_n-n&amp;amp;=\\sum_{i=1}^{n-1}H_i\\end{aligned}\\]Pulling down our earlier equality:\\[\\begin{aligned}nH_n-n&amp;amp;=nH_{n-1}-n+1\\\\H_n&amp;amp;=H_{n-1}+\\frac{1}{n}\\end{aligned}\\]Which is true by the definition of the harmonic numbers. Q.E.D.Into the Rabbit HoleWe’ve solved our initial mystery: we set out to find the average behavior, and after a false start and a few struggles, we eventually found our suspect and proved they were the murderer. But if you recall, at the beginning of our case we saw the fingerprint of someone familiar, a fingerprint indicating something deeper is at play. Why was our initial guess wrong, what distinguishes this situation from other halving algorithms? Why did the harmonic numbers show up? Can we refine our intuition?In my opinion, these are the types of questions that allow a problem solver to transition from good to great. To not only answer a question, but to really solidify our understanding of that answer. To challenge ourselves to thoroughly eviscerate the shroud of mystery.These questions all have their own rich answers that I think are best found through exploration. For example, in attempting to deduce why we found the harmonic numbers here, we might wonder about similar stochastic processes, leading us into the realm of absorbing Markov chains, where geometric series are plentiful. Or maybe for repairing our intuition, we might try to visualize every possible path, and see if we can pick up any insight from the resulting asymmetries.So, with all that said, let’s all try to have a little more fun with our problem solving and our explorations: challenges are the best teachers, fun challenges even more so." } ]
